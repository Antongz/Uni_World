{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load python packages\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "import pandas as pd\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "import IPython\n",
    "import sklearn\n",
    "import pprint\n",
    "import json\n",
    "from pprint import pprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "numpy: 1.12.1\n",
      "scipy: 1.0.0\n",
      "matplotlib: 2.1.2\n",
      "iPython: 6.2.1\n",
      "scikit-learn: 0.19.1\n",
      "Tensorflow:  1.1.0\n",
      "keras:  2.1.5\n"
     ]
    }
   ],
   "source": [
    "# print package version \n",
    "print('numpy:', np.__version__)\n",
    "print('scipy:', sp.__version__)\n",
    "print('matplotlib:', matplotlib.__version__)\n",
    "print('iPython:', IPython.__version__)\n",
    "print('scikit-learn:', sklearn.__version__)\n",
    "print('Tensorflow: ', tf.__version__)\n",
    "print('keras: ', keras.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "choices = {\n",
    "            'salt' : 0,\n",
    "            'romaine lettuce' : 1,\n",
    "            'black olives' : 2,\n",
    "            'grape tomatoes' : 3,\n",
    "            'garlic' : 4,\n",
    "            'pepper' : 5,\n",
    "            'purple onion' : 6,\n",
    "            'seasoning' : 7,\n",
    "            'garbanzo beans' : 8,\n",
    "            'feta cheese crumbles' : 9,\n",
    "            'plain flour' : 10,\n",
    "            'ground pepper' : 11,\n",
    "            'tomatoes' : 12,\n",
    "            'ground black pepper' : 13,\n",
    "            'thyme' : 14,\n",
    "            'eggs' : 15,\n",
    "            'green tomatoes' : 16,\n",
    "            'yellow corn meal': 17,\n",
    "            'milk': 18,\n",
    "            'vegetable oil': 19,\n",
    "            'mayonaise': 20,\n",
    "            'cooking oil': 21,\n",
    "            'green chilies': 22,\n",
    "            'grilled chicken breasts': 23,\n",
    "            'garlic powder': 24,\n",
    "            'yellow onion': 25,\n",
    "            'soy sauce': 26,\n",
    "            'butter' : 27,\n",
    "            'chicken livers' : 28,\n",
    "            'water' : 29,\n",
    "            'wheat' : 30,\n",
    "            'black pepper' : 31,\n",
    "            'shallots' : 32,\n",
    "            'cornflour' : 33,\n",
    "            'cayenne pepper' : 34,\n",
    "            'onions' : 35,\n",
    "            'garlic paste' : 36,\n",
    "            'lemon juice' : 37,\n",
    "            'chili powder' : 38,\n",
    "            'passata' : 39,\n",
    "            'oil' : 40,\n",
    "            'ground cumin' : 41,\n",
    "            'boneless chicken skinless thigh' : 42,\n",
    "            'garam masala' : 43,\n",
    "            'double cream' : 44,\n",
    "            'natural yogurt' : 45,\n",
    "            'bay leaf' : 46,\n",
    "            'sugar' : 47,\n",
    "            'fresh ginger root' : 48,\n",
    "            'ground cinnamon' : 49,\n",
    "            'vanilla extract' : 50,\n",
    "            'ground ginger' : 51,\n",
    "            'powdered sugar' : 52,\n",
    "            'baking powder' : 53,\n",
    "            'olive oil' : 54,\n",
    "            'medium shrimp' : 55,\n",
    "            'chopped cilantro' : 56,\n",
    "            'jalapeno chilies' : 57,\n",
    "            'flat leaf parsley' : 58,\n",
    "            'skirt steak' : 59,\n",
    "            'white vinegar' : 60,\n",
    "            'sea salt' : 61,\n",
    "            'chorizo sausage' : 62,\n",
    "            'pistachio nuts' : 63,\n",
    "            'white almond bark' : 64,\n",
    "            'flour' : 65,\n",
    "            'almond extract' : 66,\n",
    "            'dried cranberries' : 67,\n",
    "            'fresh pineapple' : 68,\n",
    "            'poblano peppers' : 69,\n",
    "            'corn tortillas' : 70,\n",
    "            'cheddar cheese' : 71,\n",
    "            'pork' : 72,\n",
    "            'iceberg lettuce' : 73,\n",
    "            'lime' : 74,\n",
    "            'chopped cilantro fresh' : 75,\n",
    "            'chopped tomatoes' : 76,\n",
    "            'fresh basil' : 77,\n",
    "            'extra-virgin olive oil' : 78,\n",
    "            'kosher salt' : 79,\n",
    "            'pimentos' : 80,\n",
    "            'sweet pepper' : 81,\n",
    "            'dried oregano' : 82,\n",
    "            'sharp cheddar cheese' : 83,\n",
    "            'swiss cheese' : 84,\n",
    "            'provolone cheese' : 85,\n",
    "            'canola oil' : 86,\n",
    "            'mushrooms' : 87,\n",
    "            'sausages' : 88,\n",
    "            'low sodium soy sauce' : 89,\n",
    "            'fresh ginger' : 90,\n",
    "            'dry mustard' : 91,\n",
    "            'bananas' : 92,\n",
    "            'green beans' : 93,\n",
    "            'white pepper' : 94,\n",
    "            'sesame oil' : 95,\n",
    "            'scallions' : 96,\n",
    "            'Shaoxing wine' : 97,\n",
    "            'ground turkey' : 98,\n",
    "            'crushed red pepper flakes' : 99,\n",
    "            'corn starch' : 100,\n",
    "            'Italian parsley leaves' : 101,\n",
    "            'walnuts' : 102,\n",
    "            'hot red pepper flakes' : 103,\n",
    "            'fresh lemon juice' : 104,\n",
    "            'trout fillet' : 105,\n",
    "            'garlic cloves' : 106,\n",
    "            'chipotle chile' : 107,\n",
    "            'fine sea salt' : 108,\n",
    "            'fresh cilantro' : 109,\n",
    "            'plain flour' : 110,\n",
    "            'ground coriander' : 111,\n",
    "            'plum tomatoes' : 112,\n",
    "            'avocado' : 113,\n",
    "            'lime juice' : 114,\n",
    "            'flank steak' : 115,\n",
    "            'fresh parmesan cheese' : 116,\n",
    "            'all-purpose flour': 117,\n",
    "            'fat free less sodium chicken broth': 118,\n",
    "            'chopped fresh chives': 119,\n",
    "            'gruyere cheese': 120,\n",
    "            'bacon slices': 121,\n",
    "            'gnocchi': 122,\n",
    "            'fat free milk': 123,\n",
    "            'cooking spray': 124,\n",
    "            'tumeric': 125,\n",
    "            'vegetable stock': 126,\n",
    "            'naan' : 127,\n",
    "            'red lentils' : 128,\n",
    "            'red chili peppers' : 129,\n",
    "            'spinach' : 130,\n",
    "            'sweet potatoes' : 131,\n",
    "            'greek yogurt' : 132,\n",
    "            'lemon curd' : 133,\n",
    "            'confectioners sugar' : 134,\n",
    "            'raspberries' : 135,\n",
    "            'italian seasoning' : 136,\n",
    "            'broiler-fryer chicken' : 137,\n",
    "            'zesty italian dressing' : 138,\n",
    "            'hot chili' : 139,\n",
    "            'asian fish sauce' : 140,\n",
    "            'chicken broth' : 141,\n",
    "            'red bell pepper' : 142,\n",
    "            'yellow squash' : 143,\n",
    "            'garlic chili sauce' : 144,\n",
    "            'sliced green onions' : 145,\n",
    "            'broccolini' : 146,\n",
    "            'fresh lime juice' : 147,\n",
    "            'cooked rice' : 148,\n",
    "            'chicken breasts' : 149,\n",
    "            'pork loin' : 150,\n",
    "            'roasted peanuts' : 151,\n",
    "            'hoisin sauce' : 152,\n",
    "            'creamy peanut butter' : 153,\n",
    "            'chopped fresh mint' : 154,\n",
    "            'thai basil' : 155,\n",
    "            'rice' : 156,\n",
    "            'rice noodles' : 157,\n",
    "            'beansprouts' : 158,\n",
    "            'roma tomatoes' : 159,\n",
    "            'low-fat mayonnaise' : 160,\n",
    "            'baking potatoes' : 161,\n",
    "            'spicy brown mustard' : 162,\n",
    "            'sesame seeds' : 163,\n",
    "            'red pepper' : 164,\n",
    "            'yellow peppers' : 165,\n",
    "            'extra firm tofu' : 166,\n",
    "            'broccoli' : 167,\n",
    "            'orange bell pepper' : 168,\n",
    "            'arrowroot powder' : 169,\n",
    "            'red curry paste' : 170,\n",
    "            'marinara sauce' : 171,\n",
    "            'linguine' : 172,\n",
    "            'capers' : 173,\n",
    "            'olives' : 174,\n",
    "            'lemon zest' : 175,\n",
    "            'lo mein noodles' : 176,\n",
    "            'light soy sauce' : 177,\n",
    "            'dried black mushrooms' : 178,\n",
    "            'chives' : 179,\n",
    "            'oyster sauce' : 180,\n",
    "            'dark soy sauce' : 181,\n",
    "            'peanuts' : 182,\n",
    "            'cabbage' : 183,\n",
    "            'herbs' : 184,\n",
    "            'fresh tomatoes' : 185,\n",
    "            'paprika' : 186,\n",
    "            'mango' : 187,\n",
    "            'stock' : 188,\n",
    "            'chile pepper' : 189,\n",
    "            'sliced mushrooms' : 190,\n",
    "            'sherry' : 191,\n",
    "            'grated parmesan cheese' : 192,\n",
    "            'heavy cream' : 193,\n",
    "            'spaghetti' : 194,\n",
    "            'cooked chicken' : 195,\n",
    "            'green bell pepper' : 196,\n",
    "            'egg roll wrappers' : 197,\n",
    "            'sweet and sour sauce' : 198,\n",
    "            'molasses' : 199,\n",
    "            'shredded cabbage' : 200,\n",
    "            'ground pork' : 201,\n",
    "            'carrots' : 202,\n",
    "            'flour tortillas' : 203,\n",
    "            'cheese' : 204,\n",
    "            'breakfast sausages' : 205,\n",
    "            'large eggs' : 206,\n",
    "            'boiling water' : 207,\n",
    "            'hot sauce' : 208,\n",
    "            'potatoes' : 209,\n",
    "            'bacon' : 210,\n",
    "            'fresh parsley' : 211,\n",
    "            'andouille sausage' : 212,\n",
    "            'cajun seasoning' : 213,\n",
    "            'peanut oil' : 214,\n",
    "            'celery' : 215,\n",
    "            'ground red pepper' : 216,\n",
    "            'shrimp' : 217,\n",
    "            'onion powder' : 218,\n",
    "            'firm tofu' : 219\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "with open('train.json') as f:\n",
    "    train = json.load(f)\n",
    "\n",
    "# 39774 'train samples'. Half 19887\n",
    "\n",
    "size_train = math.ceil(len(train)/2)\n",
    "x_train = [[None] for i in range(size_train)]\n",
    "y_train = [[None] for i in range(size_train)]\n",
    "\n",
    "for i in range(size_train):\n",
    "    result = {\n",
    "        'vietnamese' : 0,\n",
    "        'brazilian' : 1,\n",
    "        'british' : 2,\n",
    "        'cajun_creole' : 3,\n",
    "        'chinese' : 4,\n",
    "        'filipino' : 5,\n",
    "        'french' : 6,\n",
    "        'greek' : 7,\n",
    "        'indian' : 8,\n",
    "        'irish' : 9,\n",
    "        'italian' : 10,\n",
    "        'jamaican' : 11,\n",
    "        'japanese' : 12,\n",
    "        'korean' : 13,\n",
    "        'mexican' : 14,\n",
    "        'moroccan' : 15,\n",
    "        'russian' : 16,\n",
    "        'southern_us': 17,\n",
    "        'spanish': 18,\n",
    "        'thai': 19,\n",
    "    }[train[i]['cuisine']]    \n",
    "    \n",
    "    y_train[i] = result\n",
    "    \n",
    "    #print y_train[i]\n",
    "    \n",
    "for i in range(size_train):\n",
    "    x_train[i] =[ 0 for n in range(221)]\n",
    "    for j in range(len(train[i]['ingredients'])):\n",
    "        result = choices.get(train[i]['ingredients'][j], 220)     \n",
    "        x_train[i][result] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('train.json') as f:\n",
    "    test = json.load(f)\n",
    "\n",
    "# 39774 'train samples'. Half 19887\n",
    "\n",
    "size_test = math.ceil(len(test)/2)\n",
    "x_test = [[None] for i in range(size_test)]\n",
    "y_test = [[None] for i in range(size_test)]\n",
    "\n",
    "for i in range(size_test):\n",
    "    result = {\n",
    "        'vietnamese' : 0,\n",
    "        'brazilian' : 1,\n",
    "        'british' : 2,\n",
    "        'cajun_creole' : 3,\n",
    "        'chinese' : 4,\n",
    "        'filipino' : 5,\n",
    "        'french' : 6,\n",
    "        'greek' : 7,\n",
    "        'indian' : 8,\n",
    "        'irish' : 9,\n",
    "        'italian' : 10,\n",
    "        'jamaican' : 11,\n",
    "        'japanese' : 12,\n",
    "        'korean' : 13,\n",
    "        'mexican' : 14,\n",
    "        'moroccan' : 15,\n",
    "        'russian' : 16,\n",
    "        'southern_us': 17,\n",
    "        'spanish': 18,\n",
    "        'thai': 19,\n",
    "    }[test[i+19887]['cuisine']]    \n",
    "    \n",
    "    y_test[i] = result\n",
    "    \n",
    "    #print y_train[i]\n",
    "    \n",
    "for i in range(size_test):\n",
    "    x_test[i] =[ 0 for n in range(221)]\n",
    "    for j in range(len(test[i+size_test]['ingredients'])):\n",
    "        result = choices.get(test[i+size_test]['ingredients'][j], 220)     \n",
    "        x_test[i][result] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 9944, 'test samples'\n",
    "with open('test.json') as f:\n",
    "    untest = json.load(f)\n",
    "size_untest = len(untest)\n",
    "x_untest = [[None] for i in range(size_test)]\n",
    "    \n",
    "for i in range(len(untest)):\n",
    "    x_untest[i] =[ 0 for n in range(221)]\n",
    "    for j in range(len(untest[i]['ingredients'])):\n",
    "        result = choices.get(untest[i]['ingredients'][j], 220)     \n",
    "        x_untest[i][result] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import keras packages\n",
    "x_train = np.array(x_train)\n",
    "x_test = np.array(x_test)\n",
    "y_train = np.array(y_train)\n",
    "y_test = np.array(y_test)\n",
    "\n",
    "img_rows = 1\n",
    "img_cols = 221\n",
    "nmb_samples = 19887\n",
    "nmb_test_samples = 19887"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nmb_samples: 19887\n",
      "img_rows: 1\n",
      "img_cols: 221\n",
      "x_train shape: (19887, 221)\n",
      "19887 train samples\n",
      "19887 test samples\n"
     ]
    }
   ],
   "source": [
    "# type casting and dimensionality transformations\n",
    "x_train = x_train.astype('float32')\n",
    "x_test = x_test.astype('float32')\n",
    "x_train = x_train.reshape(nmb_samples, img_rows*img_cols)\n",
    "x_test = x_test.reshape(nmb_test_samples, img_rows*img_cols)\n",
    "print('nmb_samples:', nmb_samples)\n",
    "print('img_rows:', img_rows)\n",
    "print('img_cols:', img_cols)\n",
    "print('x_train shape:', x_train.shape)\n",
    "print(x_train.shape[0], 'train samples')\n",
    "print(x_test.shape[0], 'test samples')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import keras packages\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Flatten\n",
    "from keras import backend as K\n",
    "from keras.layers.convolutional import Conv2D, MaxPooling2D "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# batch size for gradient descent\n",
    "batch_size = 32\n",
    "# number of classes\n",
    "num_classes = 2\n",
    "# number of epochs (1 epoch = amount of iterations that covers the whole training set)\n",
    "epochs = 200 # try a larger number of epochs here (for example 10 or larger)\n",
    "# input image dimensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# define MLP model (2 hidden layers with 512 nodes, activated by ReLUs)\n",
    "model = Sequential()\n",
    "model.add(Dense(512, activation='relu', input_shape=(img_rows*img_cols,)))\n",
    "model.add(Dense(512, activation='relu'))\n",
    "model.add(Dense(num_classes, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compile model\n",
    "model.compile(loss=keras.losses.categorical_crossentropy,\n",
    "              optimizer=keras.optimizers.Adadelta(),\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Error when checking target: expected dense_3 to have shape (2,) but got array with shape (1,)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-714ea99f032f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m               \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m               \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m               validation_data=(x_test, y_test))\n\u001b[0m",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/keras/models.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m    961\u001b[0m                               \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    962\u001b[0m                               \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 963\u001b[0;31m                               validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m    964\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    965\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m   1628\u001b[0m             \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1629\u001b[0m             \u001b[0mclass_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mclass_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1630\u001b[0;31m             batch_size=batch_size)\n\u001b[0m\u001b[1;32m   1631\u001b[0m         \u001b[0;31m# Prepare validation data.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1632\u001b[0m         \u001b[0mdo_validation\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36m_standardize_user_data\u001b[0;34m(self, x, y, sample_weight, class_weight, check_array_lengths, batch_size)\u001b[0m\n\u001b[1;32m   1478\u001b[0m                                     \u001b[0moutput_shapes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1479\u001b[0m                                     \u001b[0mcheck_batch_axis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1480\u001b[0;31m                                     exception_prefix='target')\n\u001b[0m\u001b[1;32m   1481\u001b[0m         sample_weights = _standardize_sample_weights(sample_weight,\n\u001b[1;32m   1482\u001b[0m                                                      self._feed_output_names)\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36m_standardize_input_data\u001b[0;34m(data, names, shapes, check_batch_axis, exception_prefix)\u001b[0m\n\u001b[1;32m    121\u001b[0m                             \u001b[0;34m': expected '\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mnames\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m' to have shape '\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    122\u001b[0m                             \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m' but got array with shape '\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 123\u001b[0;31m                             str(data_shape))\n\u001b[0m\u001b[1;32m    124\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    125\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Error when checking target: expected dense_3 to have shape (2,) but got array with shape (1,)"
     ]
    }
   ],
   "source": [
    "# training\n",
    "history = model.fit(x_train, y_train,\n",
    "              batch_size=batch_size,\n",
    "              epochs=epochs,\n",
    "              verbose=1,\n",
    "              validation_data=(x_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print train and test losses and classification accuracies\n",
    "score = model.evaluate(x_train, y_train, verbose=0)\n",
    "print('Train loss:', score[0])\n",
    "print('Train accuracy:', score[1])\n",
    "score = model.evaluate(x_test, y_test, verbose=0)\n",
    "print('Test loss:', score[0])\n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# summarize history for accuracy\n",
    "plt.plot(history.history['acc'])\n",
    "plt.plot(history.history['val_acc'])\n",
    "plt.title('model accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()\n",
    "# summarize history for loss\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train shape: (19887, 1, 221, 1)\n",
      "x_test shape: (19887, 1, 221, 1)\n",
      "19887 train samples\n",
      "19887 test samples\n",
      "input shape: (1, 221, 1)\n"
     ]
    }
   ],
   "source": [
    "if K.image_data_format() == 'channels_first':\n",
    "    x_train = x_train.reshape(x_train.shape[0], 1, img_rows, img_cols)\n",
    "    x_test = x_test.reshape(x_test.shape[0], 1, img_rows, img_cols)\n",
    "    input_shape = (1, img_rows, img_cols)\n",
    "else:\n",
    "    x_train = x_train.reshape(x_train.shape[0], img_rows, img_cols, 1)\n",
    "    x_test = x_test.reshape(x_test.shape[0], img_rows, img_cols, 1)\n",
    "    input_shape = (img_rows, img_cols, 1)\n",
    "\n",
    "# type casting and dimensionality transformations\n",
    "x_train = x_train.astype('float32')\n",
    "x_test = x_test.astype('float32')\n",
    "y_train = y_train.astype('float32')\n",
    "y_test = y_test.astype('float32')\n",
    "\n",
    "print('x_train shape:', x_train.shape)\n",
    "print('x_test shape:', x_test.shape)\n",
    "print(x_train.shape[0], 'train samples')\n",
    "print(x_test.shape[0], 'test samples')\n",
    "print ('input shape:', input_shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Conv2D' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-22-263f61d54464>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# CNN model (2 hidden layers with 512 nodes)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSequential\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m model.add(Conv2D(32, kernel_size=(3, 3),\n\u001b[0m\u001b[1;32m      4\u001b[0m                  \u001b[0mactivation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'relu'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m                  input_shape=input_shape))\n",
      "\u001b[0;31mNameError\u001b[0m: name 'Conv2D' is not defined"
     ]
    }
   ],
   "source": [
    "# CNN model (2 hidden layers with 512 nodes)\n",
    "model = Sequential()\n",
    "model.add(Conv2D(32, kernel_size=(3, 3),\n",
    "                 activation='relu', \n",
    "                 input_shape=input_shape))\n",
    "model.add(Conv2D(64, (3,3), activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(512, activation='relu'))\n",
    "model.add(Dense(num_classes, activation='softmax'))\n",
    "\n",
    "# compile model\n",
    "model.compile(loss=keras.losses.categorical_crossentropy,\n",
    "              optimizer=keras.optimizers.Adadelta(),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# training\n",
    "history = model.fit(x_train, y_train,\n",
    "              batch_size=batch_size,\n",
    "              epochs=epochs,\n",
    "              verbose=1,\n",
    "              validation_data=(x_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print train and test losses and classification accuracies\n",
    "score = model.evaluate(x_train, y_train, verbose=0)\n",
    "print('Train loss:', score[0])\n",
    "print('Train accuracy:', score[1])\n",
    "score = model.evaluate(x_test, y_test, verbose=0)\n",
    "print('Test loss:', score[0])\n",
    "print('Test accuracy:', score[1])\n",
    "\n",
    "# summarize history for accuracy\n",
    "plt.plot(history.history['acc'])\n",
    "plt.plot(history.history['val_acc'])\n",
    "plt.title('model accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()\n",
    "\n",
    "# summarize history for loss\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
